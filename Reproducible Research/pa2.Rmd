---
title: ''
author: "Dustin L. Hodge"
date: "Monday, October 19, 2015"
output: html_document
---

# Synopsis
(at most 10 completely sentences summarizing analysis)

# Data Processing

```{r echo=FALSE}
library(magrittr)
library(dplyr)
library(lubridate)
```

## Reading in the Data

Downloading, unpacking, and loading in CSV data file.
```{r cache=TRUE}
# setting once to avoid typos
fileName <- "StormData.bz2"
# download the data if it's not in the current directory
if(!file.exists(fileName)) 
    download.file("https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2",
                  fileName, "libcurl")
rawStormData <- read.csv(fileName)
rm(fileName) #don't need this variable anymore
```

## Smart Subsetting

### By Geography

We're only concerned with storm effects in the United States proper, not the various islands and territories, so it's a good idea to eliminate them from the dataset under analysis. To do this, we subset the raw data based on the provided FIPS codes in column `STATE_`.

```{r}
stateStormData <- rawStormData[rawStormData$STATE_ <= 56,]
```

It's not immediately obvious where the number `56` comes from because there are 50 states. 51 was my first guess (to include the District of Colmbia), but according to the official [FIPS table](http://www.columbia.edu/~sue/state-fips.html), it's 56. (If you look closely, 5 numbers are omitted entirely; I'm sure there's a story there, but that's someone else's project!)

### By Date

A lot of the data here is really old. There's a few reasons to be concerned about this:

 - Inflation rates are a big deal. A $1M storm today is not the same as a $1M storm in 1955.
 - Methodologies for assessing storm damage may well have changed in the intervening years.
 
We want a large enough subset of the data that we can be sure that we're getting a feel that we can generalize historically, but not so large that the above are issues. The turn of the centry is a psychologically statisfying number, we'll just have to hope it's statistically satisfying, too.

```{r}
stateStormData$BGN_DATE <- parse_date_time(stateStormData$BGN_DATE, "m/d/y H:M:S")
relStateStormData <- stateStormData[year(stateStormData$BGN_DATE) > 2000,]
```

## Collapsing Events

The `EVTYPE` field, representing the weather events we care about, is rather chaotic.

```{r}
relStateStormData %>% group_by(EVTYPE) %>% summarize(no_rows = length(EVTYPE))
```

159 distinct entries! Clearly there are typos and silly entires and different casing and lions and tigers and bears. What follows is a series of simple transformations to try to categorize things in a more manageable way. In general, what we're doing is looking to see if any low-occurence entries are very similar to any high-occurence entries, and then we collapse them together. The official National Weather Service guidelines for events serve as a guide, but for ease of understanding we aim to be less granular than that. Inline comments will further explain reasoning where appropriate. (You are forgiven for falling asleep through this.)

```{r}
# first, trimming out whitespace and enforcing consistent capitalization
relStateStormData$EVTYPE <- trimws(toupper(relStateStormData$EVTYPE))

# -------remove noise-------------
# these rows are just weird or indecipherable
relStateStormData <- relStateStormData[!(relStateStormData$EVTYPE %in%
                                             c("OTHER","NORTHERN LIGHTS", "RED FLAG CRITERIA")),]

# -----same difference!-----------
# collapsing categories into ones I think make sense; all the manipulation is listed here
# in case anyone disagrees with my refinements

# COASTAL FLOODING/TIDES
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("COASTAL FLOOD","BEACH EROSION","CSTL FLOODING/EROSION",
                               "HEAVY SURF/HIGH SURF","HIGH SURF","HEAVY SURF",
                               "LAKESHORE FLOOD","HAZARDOUS SURF","HIGH WATER",
                               "STORM SURGE/TIDE","ASTRONOMICAL LOW TIDE","ASTRONOMICAL HIGH TIDE",
                               "STORM SURGE")] <- "COASTAL FLOODING/TIDES"

# FREEZING TEMPERATURES/WIND CHILL
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("COLD","COLD WEATHER","COLD WIND CHILL TEMPERATURES",
                               "UNSEASONABLY COLD","UNSEASONABLY COOL","UNUSUALLY COLD",
                               "FROST/FREEZE","EXTREME COLD/WIND CHILL","EXTREME COLD",
                               "EXTREME WINDCHILL","EXTREME WINDCHILL TEMPERATURE",
                               "FREEZE","FROST","HARD FREEZE","PROLONG COLD","RECORD COLD",
                               "EXTREME WINDCHILL TEMPERATURES")] <- "FREEZING TEMPERATURES/WIND CHILL"

# FLOOD
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("ABNORMALLY WET","DROWNING","URBAN/SML STREAM FLD",
                               "EXTREMELY WET","SNOWMELT FLOODING")] <- "FLOOD"
# DROUGHT
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("ABNORMALLY DRY","DRY","DRY CONDITIONS","DRY SPELL",
                               "UNSEASONABLY DRY","VERY DRY","RECORD LOW RAINFALL",
                               "SNOW DROUGHT","EXCESSIVE HEAT/DROUGHT")] <- "DROUGHT"

# HIGH WIND
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("WND","STRONG WIND","WIND","STRONG WINDS","WIND ADVISORY",
                               "WIND DAMAGE","GRADIENT WIND")] <- "HIGH WIND"

# DUST STORM
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("BLOWING DUST")] <- "DUST STORM"

# SNOW
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("ACCUMULATED SNOWFALL","EARLY SNOWFALL","FIRST SNOW",
                               "LAKE-EFFECT SNOW","LAKE EFFECT SNOW","LATE SEASON SNOW",
                               "LIGHT SNOW","MODERATE SNOWFALL","SNOW ADVISORY","SNOW SHOWERS",
                               "SNOW SQUALLS","SNOW/BLOWING SNOW","UNUSUALLY LATE SNOW","HEAVY SNOW")] <- "SNOW"

# DUST DEVIL
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("DUST DEVEL")] <- "DUST DEVIL"

# TORNADO
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("FUNNEL CLOUD","WHIRLWIND","FUNNEL CLOUDS")] <- "TORNADO"

# HIGH TEMPERATURES
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("EXCESSIVE HEAT","HEAT","RECORD HEAT")] <- "HIGH TEMPERATURES"

# SLEET
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c()] <- "SLEET"

# FREEZING RAIN
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("FREEZING DRIZZLE")] <- "FREEZING RAIN"

# WINTRY MIX
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("FALLING SNOW/ICE","ICE/SNOW","SNOW/FREEZING RAIN","SNOW/SLEET",
                               "MIXED PRECIPITATION","WINTER WEATHER MIX","WINTER WEATHER/MIX")] <- "WINTRY MIX"

# ICE
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("BLACK ICE", "ICE ON ROAD","PATCHY ICE","ICY ROADS")] <- "ICY CONDITIONS"

# FOG
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("DENSE FOG")] <- "FOG"

```

### By Relevant Data

Next, we need to reduce the data set to the observations that we care about with respect to each question. `EVTYPE` is important for both pieces of analysis but, now that we've already restricted the dataset by geography, most of the other 36 columns are just noise. For determining effects of various storms on population health, we only need to look at the `FATALITIES` and `INJURIES` columns.

```{r}
popHealthSubset <- subset(stateStormData, select = c(EVTYPE, FATALITIES, INJURIES))

```

And, luckily, there are no missing values in the dataset.

```{r}
sum(is.na(popHealthSubset$FATALITIES), is.na(popHealthSubset$INJURIES))
```

The subset we need for assessing economic consequences is slightly bigger, but not by much. The columns `PROPDMG` and `PROPDMGXP` together explain damaged to property;likewise `CROPDMG` and `CROPDMGEXP` explain damage to crops (in terms of dollar value loss).

```{r}
dmgSubset <- subset(stateStormData, select = c(EVTYPE, PROPDMG, PROPDMGEXP, CROPDMG, CROPDMGEXP))

```

And, again, we thank our lucky starts that we don't have to deal with any missing data.

```{r}
sum(is.na(dmgSubset$CROPDMG), is.na(dmgSubset$PROPDMG))
```

### Showing You the Money

No further processing is needed on the health subset, but the damage subset is a little strange. Likely in an effort to save a little space, instead of listing raw dollar values in one column the data set has each value represented in *two* columns. The `*DMG` columns have raw decimal numbers but `*DMGEXP` columns have a coded multiplier. For example, if, for a given obsevation, there's the number `25.00` in the `PROPDMG` column and the symbol `K` in the `PROPDMGEXP` column, K is an abbreviation for `1000`. So, for this example, the storm would have caused $25,000 in damage. It's relatively human-readable because the chosen codes are common abbreviations for large numbers, but it's not computer readable. That won't do for making pretty graphs!

However, this is also when we start to have to deal some data entry errors. Officially, 'H', 'K', 'M', and 'B' (standing for 'Hundreds,' 'Thousands,' 'Millions,' and 'Billions,' respectively) and a blank (meaning a mult. of 1) are supposed to be the only codes. But if we examine all the factors for the `PROPDMGEXP` variable in the `dmgSubset` data, we see this isn't quite the case. (The other data set has some similar weirdness, but not nearly as much.)

```{r}
dmgSubset %>% group_by(PROPDMGEXP) %>% summarize(no_rows = length(PROPDMGEXP))
```

Capitalization errors are easy to deal with. The rest, though... One row may only be a $20 line-item. With almost 900K observations that doesn't matter so much. Or it may be a *$20B* line-item, and then it matters *a lot*! For the most part, I'm deciding to omit the observations because, if I can't tell the intent then I shouldn't assume it. Here's what I will be including in the analysis and why:

  - **lowercase codes** - This intent here is obvious, but I'm calling it out to be thorough. These will be treated exactly like their uppercase brethren.
 - **"0", "1"** - For the numerals, the intent is fairly easy to presume. These will be treated like a multiplier of 1. 
 - **"-", "+"** - This is a tougher call. However, the '+' and '-' are so close to the number 0 on most keyboards - and there are so few of these type of entries - that I'm going to take the risk and assume the intent was a multiplier of 1.
 - **all other codes**  - These will be omitted, by zeroing out the data in a piecemeal fashion. i.e, I'm not going to drop the entire row, because there could be good crop data where there's bad property data and vice versa.
 
Without further adieu, let's replace the codes with numbers.

```{r}
# -------Property Values--------------
# for this to work, we need to see these as characters not factors
# that's ok, we won't need the factors anymore anyway
dmgSubset$PROPDMGEXP <- as.character(dmgSubset$PROPDMGEXP)
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('','0','1','-','+')] <- '1'
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('h', 'H')] <- '100'
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('k', 'K')] <- '1000'
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('m', 'M')] <- '1000000'
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('b', 'B')] <- '1000000000'
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('?', '2', '3', '4', '5', '6', '7', '8')] <- '0'
# back to numeric so we can do math
dmgSubset$PROPDMGEXP <- as.integer(dmgSubset$PROPDMGEXP)

# -------Crop Values------------------
dmgSubset$CROPDMGEXP <- as.character(dmgSubset$CROPDMGEXP)
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('','0')] <- '1'
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('h', 'H')] <- '100'
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('k', 'K')] <- '1000'
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('m', 'M')] <- '1000000'
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('b', 'B')] <- '1000000000'
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('?', '2')] <- '0'
dmgSubset$CROPDMGEXP <- as.integer(dmgSubset$PROPDMGEXP)

```

Last thing to do is make a few new columns to reprepsent the total property damage and total crop damage, respectively, for each entry.

```{r}
dmgSubset <- mutate(dmgSubset, PROPTOT = PROPDMG * PROPDMGEXP,
                               CROPTOT = CROPDMG * CROPDMGEXP)
```


# Results
Questions:
1. Across the United States, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?

2. Across the United States, which types of events have the greatest economic consequences?