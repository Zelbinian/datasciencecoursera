---
title: 'The Effects of Various Weather Events on Public Health and the Economy'
author: "Dustin L. Hodge"
date: "Monday, October 19, 2015"
output: html_document
---

# Synopsis
(at most 10 completely sentences summarizing analysis)

# Data Processing

```{r echo=FALSE, message=FALSE}
library(magrittr)
library(dplyr)
library(lubridate)
```

## Reading in the Data

Downloading, unpacking, and loading in CSV data file.
```{r cache=TRUE}
# setting once to avoid typos
fileName <- "StormData.bz2"
# download the data if it's not in the current directory
if(!file.exists(fileName)) 
    download.file("https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2",
                  fileName, "libcurl")
rawStormData <- read.csv(fileName)
rm(fileName) #don't need this variable anymore
```

## Smart Subsetting

### By Geography

We're only concerned with storm effects in the United States proper, not the various islands and territories, so it's a good idea to eliminate them from the dataset under analysis. To do this, we subset the raw data based on the provided FIPS codes in column `STATE_`.

```{r}
stateStormData <- rawStormData[rawStormData$STATE_ <= 56,]
```

It's not immediately obvious where the number `56` comes from because there are 50 states. 51 was my first guess (50 states + the District of Colmbia), but according to the official [FIPS table](http://www.columbia.edu/~sue/state-fips.html), it's 56. (If you look closely, 5 numbers are omitted entirely; I'm sure there's a story there, but that's someone else's project!)

### By Date

A lot of the data here is really old. There's a few reasons to be concerned about this:

 - Inflation rates are a big deal. A $1M storm today is not the same as a $1M storm in 1955.
 - Methodologies for assessing storm damage may well have changed in the intervening years.
 
We want a large enough subset of the data that we can be sure that we're getting results that we can generalize historically, but not so large that the above are issues. The turn of the century is a psychologically statisfying number, we'll just have to hope it's statistically satisfying, too.

```{r}
stateStormData$BGN_DATE <- parse_date_time(stateStormData$BGN_DATE, "m/d/y H:M:S")
relStateStormData <- stateStormData[year(stateStormData$BGN_DATE) > 2000,]
```

## Collapsing Events

The `EVTYPE` field, representing the weather events we care about, is rather chaotic.

```{r}
relStateStormData %>% group_by(EVTYPE) %>% summarize(no_rows = length(EVTYPE))
```

Gadzooks! 159 distinct entries! Clearly there are typos and silly entires and different casing and lions and tigers and bears. What follows is a series of simple transformations to try to categorize things in a more manageable way. In general, what we're doing is looking to see if any low-occurence entries are very similar to any high-occurence entries, and then we collapse them together. The official National Weather Service guidelines for events serve as a guide, but for ease of understanding we aim to be less granular than that. Inline comments will further explain reasoning where appropriate. (You are forgiven for falling asleep through this.)

```{r}
# unfactoring the EVTYPE variable so this will all work
relStateStormData$EVTYPE <- as.character(relStateStormData$EVTYPE)

# first, trimming out whitespace and enforcing consistent capitalization
relStateStormData$EVTYPE <- trimws(toupper(relStateStormData$EVTYPE))

# -------remove noise-------------
# these rows are just weird or indecipherable
relStateStormData <- relStateStormData[!(relStateStormData$EVTYPE %in%
                                             c("OTHER","NORTHERN LIGHTS", "RED FLAG CRITERIA")),]

# -----same difference!-----------
# collapsing categories into ones I think make sense; all the manipulation is listed here
# in case anyone disagrees with my refinements

# COASTAL FLOODING/TIDES
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("COASTAL FLOOD","BEACH EROSION","CSTL FLOODING/EROSION",
                               "HEAVY SURF/HIGH SURF","HIGH SURF","HEAVY SURF",
                               "LAKESHORE FLOOD","HAZARDOUS SURF","HIGH WATER",
                               "STORM SURGE/TIDE","ASTRONOMICAL LOW TIDE","ASTRONOMICAL HIGH TIDE",
                               "STORM SURGE","COASTAL FLOODING","ROGUE WAVE")] <- "COASTAL FLOODING/TIDES"

# FREEZING TEMPERATURES/WIND CHILL
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("COLD","COLD WEATHER","COLD WIND CHILL TEMPERATURES",
                               "UNSEASONABLY COLD","UNSEASONABLY COOL","UNUSUALLY COLD",
                               "FROST/FREEZE","EXTREME COLD/WIND CHILL","EXTREME COLD",
                               "EXTREME WINDCHILL","EXTREME WINDCHILL TEMPERATURE",
                               "FREEZE","FROST","HARD FREEZE","PROLONG COLD","RECORD COLD",
                               "EXTREME WINDCHILL TEMPERATURES","COLD/WIND CHILL","WIND CHILL")] <- "FREEZING TEMPERATURES/WIND CHILL"

# FLOOD
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("ABNORMALLY WET","DROWNING","URBAN/SML STREAM FLD",
                               "EXTREMELY WET","SNOWMELT FLOODING")] <- "FLOOD"
# HEAVY RAIN
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("RAIN","HEAVY RAIN EFFECTS","MONTHLY PRECIPITATION")] <- "HEAVY RAIN"

# MUDSLIDE
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("MUD SLIDE")] <- "MUDSLIDE"

# RIP CURRENT
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("RIP CURRENTS", "RIPCURRENT")] <- "RIP CURRENT"

# DROUGHT
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("ABNORMALLY DRY","DRY","DRY CONDITIONS","DRY SPELL",
                               "UNSEASONABLY DRY","VERY DRY","RECORD LOW RAINFALL",
                               "SNOW DROUGHT","EXCESSIVE HEAT/DROUGHT")] <- "DROUGHT"

# HIGH WIND
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("WND","STRONG WIND","WIND","STRONG WINDS","WIND ADVISORY",
                               "WIND DAMAGE","GRADIENT WIND","GUSTY LAKE WIND","GUSTY WIND",
                               "NON-TSTM WIND","NON TSTM WIND","GUSTY WINDS")] <- "HIGH WIND"

# DUST STORM
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("BLOWING DUST")] <- "DUST STORM"

# SNOW
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("ACCUMULATED SNOWFALL","EARLY SNOWFALL","FIRST SNOW",
                               "LAKE-EFFECT SNOW","LAKE EFFECT SNOW","LATE SEASON SNOW",
                               "LIGHT SNOW","MODERATE SNOWFALL","SNOW ADVISORY","SNOW SHOWERS",
                               "SNOW SQUALLS","SNOW/BLOWING SNOW","UNUSUALLY LATE SNOW","HEAVY SNOW")] <- "SNOW"

# DUST DEVIL
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("DUST DEVEL")] <- "DUST DEVIL"

# TORNADO
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("FUNNEL CLOUD","WHIRLWIND","FUNNEL CLOUDS","TORNADO DEBRIS",
                               "WATERSPOUT")] <- "TORNADO"

# HIGH TEMPERATURES
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("EXCESSIVE HEAT","HEAT","RECORD HEAT","UNSEASONABLY HOT",
                               "PROLONG WARMTH","RECORD WARMTH","UNSEASONABLY WARM",
                               "VERY WARM")] <- "HIGH TEMPERATURES"

# HAIL
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("SMALL HAIL")] <- "HAIL"

# ICY CONDITIONS
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("BLACK ICE", "ICE ON ROAD","PATCHY ICE","ICY ROADS")] <- "ICY CONDITIONS"

# FOG
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("DENSE FOG")] <- "FOG"

# THUNDERSTORM
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("THUNDERSTORM WIND","TSTM WIND","TSTM WIND/HAIL","THUNDERSTORMS",
                               "GUSTY THUNDERSTORM WIND","GUSTY THUNDERSTORM WINDS",
                               "SEVERE THUNDERSTORMS")] <- "THUNDERSTORM"

# WINTER STORM
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("FALLING SNOW/ICE","ICE/SNOW","SNOW/FREEZING RAIN","SNOW/SLEET",
                               "MIXED PRECIPITATION","WINTER WEATHER MIX","WINTER WEATHER/MIX",
                               "WINTER WEATHER","WINTRY MIX","ICE STORM","FREEZING DRIZZLE",
                               "LIGHT FREEZING RAIN","FREEZING RAIN","SLEET STORM","SLEET")] <- "WINTER STORM"

# WILDFIRE
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("BRUSH FIRE","WILD/FOREST FIRE")] <- "WILDFIRE"

# SMOKE
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("DENSE SMOKE")] <- "SMOKE"

# TROPICAL CYCLONE 
# (this is the broader term for hurricanes, typhoons, tropical storms, etc)
relStateStormData$EVTYPE[relStateStormData$EVTYPE %in% 
                             c("HURRICANE","HURRICANE/TYPHOON","TROPICAL DEPRESSION",
                               "TROPICAL STORM")] <- "TROPICAL CYCLONE"

```

This reduces the 159 events to just 32, which are arguably logical and all clearly distinct from one another.

Now, at long last, we're ready to tackle the effects of all these weather events.

# Results

## Question 1

> Across the United States, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?

First, we need to reduce the data set to the variables germaine to this question; most of the other 37 columns are just noise. For determining effects of various storms on population health, we only need to look at `FATALITIES` and `INJURIES` (and, of course, `EVTYPE`).

```{r}
popHealthSubset <- subset(relStateStormData, select = c(EVTYPE, FATALITIES, INJURIES))

```

Luckily, there are no missing values in the dataset, so there's very little additional processing we have to do.

```{r}
sum(is.na(popHealthSubset$FATALITIES), is.na(popHealthSubset$INJURIES))
```

There is one last consideration before plotting results: we have data for both fatalities and injuries, but we want to represent the *total* harm to human health as one data point. Additionally, it's not controversial to weight fatalities as more serious consequences than just injuries; a weather event that resulted in 200 fatalities is more harmful than one with 200 injuries and the data should reflect this. After playing with the data, the simple equation `harm = INJURIES + FATALITIES * 1.25` feels about right. Now let's see what the top 10 most and least injurious weather events are.

```{r}
# generate the event -> harm index table
weatherHarmIndex <- popHealthSubset %>% group_by(EVTYPE) %>% 
    summarize(HARM_INDEX = sum(INJURIES + FATALITIES * 1.25))

# sort from most harmful to least
weatherHarmIndex <- weatherHarmIndex[with(weatherHarmIndex, order(HARM_INDEX, decreasing = TRUE)),]

# grab most and least harmful events
n <- nrow(weatherHarmIndex)
mostHarmfulEvents <- weatherHarmIndex[1:10,]
leastHarmfulEvents <- weatherHarmIndex[(n-9):n,]

# plot

```

## Question 2

> Across the United States, which types of events have the greatest economic consequences?

The subset we need for assessing economic consequences is slightly bigger, but not by much. The columns `PROPDMG` and `PROPDMGXP` together explain damaged to property;likewise `CROPDMG` and `CROPDMGEXP` explain damage to crops (in terms of dollar value loss).

```{r}
dmgSubset <- subset(stateStormData, select = c(EVTYPE, PROPDMG, PROPDMGEXP, CROPDMG, CROPDMGEXP))

```

And, again, we thank our lucky stars that we don't have to deal with any missing data.

```{r}
sum(is.na(dmgSubset$CROPDMG), is.na(dmgSubset$PROPDMG))
```


Subsetting for the health/harm data was straightforward, but the damage subset is a little strange. Likely in an effort to save a little space, instead of listing raw dollar values in one column the data set has each value represented in *two* columns. The `*DMG` columns have raw decimal numbers but `*DMGEXP` columns have a coded multiplier. For example, if, for a given obsevation, there's the number `25.00` in the `PROPDMG` column and the symbol `K` in the `PROPDMGEXP` column, K is an abbreviation for `1000`. So, for this example, the storm would have caused $25,000 in damage. It's relatively human-readable because the chosen codes are common abbreviations for large numbers, but it's not computer readable. That won't do for making pretty graphs!

However, this is also when we start to have to deal some data entry errors. Officially, 'H', 'K', 'M', and 'B' (standing for 'Hundreds,' 'Thousands,' 'Millions,' and 'Billions,' respectively) and a blank (meaning a mult. of 1) are supposed to be the only codes. But if we examine all the factors for the `PROPDMGEXP` variable in the `dmgSubset` data, we see this isn't quite the case. (The other data set has some similar weirdness, but not nearly as much.)

```{r}
dmgSubset %>% group_by(PROPDMGEXP) %>% summarize(no_rows = length(PROPDMGEXP))
```

Capitalization errors are easy to deal with. The rest, though... One row may only be a $20 line-item. With almost 900K observations that doesn't matter so much. Or it may be a *$20B* line-item, and then it matters *a lot*! For the most part, I'm deciding to omit the observations because, if I can't tell the intent then I shouldn't assume it. Here's what I will be including in the analysis and why:

  - **lowercase codes** - This intent here is obvious, but I'm calling it out to be thorough. These will be treated exactly like their uppercase brethren.
 - **"0", "1"** - For the numerals, the intent is fairly easy to presume. These will be treated like a multiplier of 1. 
 - **"-", "+"** - This is a tougher call. However, the '+' and '-' are so close to the number 0 on most keyboards - and there are so few of these type of entries - that I'm going to take the risk and assume the intent was a multiplier of 1.
 - **all other codes**  - These will be omitted, by zeroing out the data in a piecemeal fashion. i.e, I'm not going to drop the entire row, because there could be good crop data where there's bad property data and vice versa.
 
Without further adieu, let's replace the codes with numbers.

```{r}
# -------Property Values--------------
# for this to work, we need to see these as characters not factors
# that's ok, we won't need the factors anymore anyway
dmgSubset$PROPDMGEXP <- as.character(dmgSubset$PROPDMGEXP)
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('','0','1','-','+')] <- '1'
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('h', 'H')] <- '100'
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('k', 'K')] <- '1000'
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('m', 'M')] <- '1000000'
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('b', 'B')] <- '1000000000'
dmgSubset$PROPDMGEXP[dmgSubset$PROPDMGEXP %in% c('?', '2', '3', '4', '5', '6', '7', '8')] <- '0'
# back to numeric so we can do math
dmgSubset$PROPDMGEXP <- as.integer(dmgSubset$PROPDMGEXP)

# -------Crop Values------------------
dmgSubset$CROPDMGEXP <- as.character(dmgSubset$CROPDMGEXP)
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('','0')] <- '1'
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('h', 'H')] <- '100'
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('k', 'K')] <- '1000'
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('m', 'M')] <- '1000000'
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('b', 'B')] <- '1000000000'
dmgSubset$CROPDMGEXP[dmgSubset$CROPDMGEXP %in% c('?', '2')] <- '0'
dmgSubset$CROPDMGEXP <- as.integer(dmgSubset$PROPDMGEXP)

```

Last thing to do is make a few new columns to reprepsent the total property damage and total crop damage, respectively, for each entry.

```{r}
dmgSubset <- mutate(dmgSubset, PROPTOT = PROPDMG * PROPDMGEXP,
                               CROPTOT = CROPDMG * CROPDMGEXP)
```

From here the exercise is a almost mirror image of the harm exercise, above (except how the columns combine is more straightforward).
```{r}
# generate the event -> harm index table
weatherCosts <- dmgSubset %>% group_by(EVTYPE) %>% 
    summarize(TOTALCOST = sum(PROPTOT + CROPTOT))

# sort from most costly to least
weatherCosts <- weatherCosts[with(weatherCosts, order(TOTALCOST, decreasing = TRUE)),]

# grab most and least costly events
n <- nrow(weatherCosts)
mostCostlyEvents <- weatherCosts[1:10,]
leastCostlyEvents <- weatherCosts[(n-9):n,]

# plot
```
